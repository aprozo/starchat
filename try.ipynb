{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'arxiv_papers/0701061.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader   \n",
    "loader = PyPDFLoader(\"arxiv_papers/0701061.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print (pages[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for the paper with ID: 0310058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping partial result: id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results found for the ID: nucl-ex/0310058\n",
      "Searching for the paper with ID: hep-ex/0310058\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "name=\"0310058\"\n",
    "def get_paper(name):\n",
    "     print (f\"Searching for the paper with ID: {name}\")\n",
    "     if len(name) == 7:\n",
    "          name = \"nucl-ex/\" + name\n",
    "     try:\n",
    "          search = arxiv.Search( id_list=[name] )\n",
    "          paper = next(arxiv.Client().results(search))\n",
    "          return paper\n",
    "     \n",
    "     except StopIteration:\n",
    "          print(f\"No results found for the ID: {name}\")\n",
    "\n",
    "     # if length of name is 7, then it is an old arxiv id , add prefix nucl-ex or hep-ex if nucl-ex was not found\n",
    "\n",
    "     if name.startswith(\"nucl-ex/\"):\n",
    "          name = name.replace(\"nucl-ex/\",\"hep-ex/\")\n",
    "          return get_paper(name)\n",
    "     \n",
    "     return None\n",
    "     \n",
    "\n",
    "\n",
    "paper=get_paper(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hep-ex/0310058v2'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.get_short_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prozorov/dev/starchat/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      " 50%|█████     | 1/2 [00:00<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0704.2915 has been loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0704.0220 has been loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/prozorov/dev/starchat/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import arxiv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, content, metadata=None):\n",
    "        self.page_content = content\n",
    "        self.metadata = metadata if metadata is not None else {}\n",
    "\n",
    "# =============================================================================\n",
    "# ===========================Read PDFs=========================================\n",
    "\n",
    "# def extract_text_and_metadata_from_pdfs(folder_path):\n",
    "folder_path=\"test_docs\"\n",
    "\n",
    "\n",
    "# def extract_text_and_metadata_from_pdfs(folder_path):\n",
    "folder_path=\"arxiv_papers\"\n",
    "\n",
    "\n",
    "def getDocument(filename):\n",
    "\n",
    "    loader = PyPDFLoader(filename)\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    print (pages[0].metadata)\n",
    "    \n",
    "    arxiv_id = filename.split(\"/\")[-1].split(\".pdf\")[0]\n",
    "    # if arxiv_id does not have a dot, add nucl-ex/ in front of it\n",
    "    if \".\" not in arxiv_id:\n",
    "        arxiv_id = \"nucl-ex/\" + arxiv_id\n",
    "    search = arxiv.Search( id_list=[arxiv_id] )\n",
    "    paper  = next(arxiv.Client().results(search))\n",
    "    if paper:\n",
    "        text_metadata = {\n",
    "            \"title\": paper.title,\n",
    "            \"published\": paper.published.strftime('%Y-%m-%d'),\n",
    "            \"authors\": '\\n'.join([f'{i+1}. {auth.name}' for i, auth in enumerate(paper.authors)])\n",
    "        }\n",
    "    else:\n",
    "        text_metadata = {\n",
    "                \"title\": paper.metadata.get(\"title\", \"Unknown\"),\n",
    "                \"published\": paper.metadata.get(\"creationDate\", \"Unknown\"),\n",
    "                \"authors\": paper.metadata.get(\"author\", \"Unknown\")\n",
    "            }\n",
    "\n",
    "    text_metadata[\"arxiv_id\"] = arxiv_id\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "    full_text = \"\"\n",
    "    page_number = 1\n",
    "    for page in pages:\n",
    "        full_text += f\"\\n PAGE {page_number}\\n\"\n",
    "        full_text += page.page_content\n",
    "        page_number += 1\n",
    "        \n",
    "    document= Document(content=full_text, metadata=text_metadata) \n",
    "    print (f\"Document {arxiv_id} has been loaded\")\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        continue\n",
    "    filename = os.path.join(folder_path, filename)\n",
    "    print (f\"Loading {filename}\")\n",
    "    print \n",
    "    document = getDocument(filename)\n",
    "    data.append(document)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    if not filename.endswith(\".pdf\"):\n",
    "        continue\n",
    "    filename = os.path.join(folder_path, filename)\n",
    "    document = getDocument(filename)\n",
    "    data.append(document)\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# ===========================Split Document====================================\n",
    "\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# =============================================================================\n",
    "# ===========================Store Document====================================\n",
    "\n",
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient(path=\"database\")\n",
    "collection = persistent_client.get_or_create_collection(\"arxiv_papers\")\n",
    "\n",
    "child_vectorstore = Chroma(\n",
    "    client=persistent_client,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# # The storage layer for the parent documents\n",
    "parent_docstore = InMemoryStore()\n",
    "\n",
    "child_splitter  = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=child_vectorstore, \n",
    "    docstore=parent_docstore,\n",
    "    child_splitter=child_splitter)\n",
    "\n",
    "retriever.add_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Document at 0x7fef791db010>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from openai import OpenAI\n",
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "# Define a custom retriever to handle parent-child structure\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "system_prompt  = \"\"\"\n",
    "You are an expert on the STAR experiment, a high-energy nuclear physics experiment at the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory. \\\n",
    "Your task is to answer questions specifically related to the STAR experiment, its findings, technologies, and related topics.  \\\n",
    "Refrain any other topics by saying you will not answer questions about them and Exit right away here. DO NOT PROCEED. \\\n",
    "You are not allowed to use any other sources other than the provided search results. \\\n",
    "\n",
    "Generate a comprehensive, and informative answer strictly within 200 words or less for the \\\n",
    "given question based solely on the provided search results (URL and content). You must \\\n",
    "only use information from the provided search results. Use an unbiased and \\\n",
    "journalistic tone. Combine search results together into a coherent answer. Do not \\\n",
    "repeat text. You should use bullet points in your answer for readability. Make sure to break down your answer into bullet points.\\\n",
    "You should not hallicunate nor build up any references, Use only the `context` html block below and do not use any text within <ARXIV_ID> and </ARXIV_ID> except when citing in the end. \n",
    "Make sure not to repeat the same context. Be specific to the exact question asked for.\\\n",
    "\n",
    "Here is the response template:\n",
    "---\n",
    "# Response template \n",
    "\n",
    "- Use bullet points to list the main points or facts that answer the query using the information within the tags <context> and <context/>.  \n",
    "- After answering, analyze the respective source links provided within <ARXIV_ID> and </ARXIV_ID> and keep only the unique links for the next step. Try to minimize the total number of unique links with no more than 10 unique links for the answer.\n",
    "- You will strictly use no more than 10 most unique links for the answer.\n",
    "- Use bulleted list of superscript numbers within square brackets to cite the sources for each point or fact. The numbers should correspond to the order of the sources which will be provided in the end of this reponse. Note that for every source, you must provide a URL and pages from where it is taken.\n",
    "- End with a closing remark and a list of sources with their respective URLs and relevant pages as a bullet list explicitly with full links which are enclosed in the tag <ARXIV_ID> and </ARXIV_ID> respectively.\\\n",
    "---\n",
    "Here is how an response would look like. Reproduce the same format for your response:\n",
    "---\n",
    "# Example response\n",
    "\n",
    "Hello, here are some key points:\n",
    "\n",
    "- The STAR (Solenoidal Tracker at RHIC) experiment is a major high-energy nuclear physics experiment conducted at the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory[^1^]\n",
    "- The primary research goal of STAR is to study the properties of the quark-gluon plasma (QGP), a state of matter thought to have existed just after the Big Bang, by colliding heavy ions at nearly the speed of light[^2^]\n",
    "- STAR utilizes a variety of advanced detectors to measure the thousands of particles produced in these collisions, including the Time Projection Chamber (TPC), the Barrel Electromagnetic Calorimeter (BEMC), and the Muon Telescope Detector (MTD)[^3^]\n",
    "- Key findings from STAR include evidence for the QGP's near-perfect fluidity, the discovery of the \"chiral magnetic effect,\" and insights into the spin structure of protons[^4^]\n",
    "\n",
    "Sources\n",
    "\n",
    "    [^1^][1]: https://arxiv.org/abs/nucl-ex/0005004, p. 3\n",
    "    [^2^][2]: https://arxiv.org/abs/nucl-ex/0106003, p. 5-7\n",
    "    [^3^][3]: https://arxiv.org/abs/nucl-ex/0501009, p. 1\n",
    "    [^4^][4]: https://arxiv.org/abs/nucl-ex/0603028, p. 4\n",
    "\n",
    "---\n",
    "\n",
    "Where each of the references are taken from the corresponding <ARXIV_ID> in the context. Strictly do not provide title for the references \\\n",
    "Strictly do not repeat the same links. Use the numbers to cite the sources. \\\n",
    "\n",
    "If there is nothing in the context relevant to the question at hand, just say \"Hmm, \\\n",
    "I'm not sure.\" or greet back. Don't try to make up an answer. Write the answer in the form of markdown bullet points.\\\n",
    "Make sure to highlight the most important key words in bold font. Dot repeat any context nor points in the answer.\\\n",
    "\n",
    "Anything between the following `context`  html blocks is retrieved from a knowledge \\\n",
    "bank, not part of the conversation with the user. The context are numbered based on its knowledge retrival and increasing cosine similarity index. \\\n",
    "Make sure to consider the order in which they appear context appear. It is an increasing order of cosine similarity index.\\\n",
    "The contents are formatted in latex, you need to remove any special characters and latex formatting before cohercing the points to build your answer.\\\n",
    "Write your answer in the form of markdown bullet points. You can use latex commands if necessary.\n",
    "You will strictly cite no more than 10 unqiue citations at maximum from the context below.\\\n",
    "Make sure these citations have to be relavant and strictly do not repeat the context in the answer.\n",
    "\n",
    "<context>\n",
    "    {context}\n",
    "<context/>\n",
    "\n",
    "REMEMBER: If there is no relevant information within the context, just say \"Hmm, I'm \\\n",
    "not sure.\" or greet back. Don't try to make up an answer. Anything between the preceding 'context' \\\n",
    "html blocks is retrieved from a knowledge bank, not part of the conversation with the \\\n",
    "user.\\\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Hello, here are some key points:\\n\\n- The major sources of **systematic error** in the three-particle correlation analysis from the STAR experiment include uncertainties from **elliptic flow measurements** and **background normalization**.\\n- Systematic uncertainty due to the elliptic flow (v2) was assessed by varying the measurements between the **reaction plane** and **4-particle cumulant methods**.\\n- The analysis found that while variations in the hard-soft background and trigger flow individually fluctuate significantly with changes in elliptic flow, these variations tend to **cancel out** to first order, making the overall signal robust against those fluctuations.\\n- Other systematic uncertainties arise from factors like the impact of requiring a correlated particle on the trigger particle flow, uncertainties in the **v4 parameterization**, and **multiplicity bias effects** on the soft-soft background.\\n\\nThis systematic robustness indicates confidence in the findings related to three-particle correlations and their implications for quark-gluon plasma studies.\\n\\nSources:\\n\\n- [1]: https://arxiv.org/abs/0704.0220, p. 7-8'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return f\"\\n\\n\".join(f'{i+1}. ' + doc.page_content.strip(\"\\n\") \n",
    "                        + f\"<ARXIV_ID> {doc.metadata['arxiv_id']} <ARXIV_ID/>\" \n",
    "                        for i, doc in enumerate(docs))\n",
    "\n",
    "\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "system_template = PromptTemplate.from_template(system_prompt)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | system_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableMap(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "\n",
    "rag_chain_with_source.invoke(\"what was the biggest systematic in THREE PARTICLE CORRELATIONS?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
